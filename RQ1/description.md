We want to investigate the frequency in which AI agents contribute tests to their respective projects and repo’s. Considering that AI agents pose a risk of contributing code that contains errors, it is important for some form of a safeguard or error check to take place on their code. We would like to explore the rate at which AI agents contribute tests in relation to changes made to the codebase. You can call this the test-to-code churn ratio. If time permits, then more details will be explored, such as the metrics for the different types of tests (unit, integration etc.) contributed, and details surrounding any follow-up work for the absence of tests.  
  
**Methodologies:**
	We will use pandas to explore the various tables in the dataset. Firstly, the PR’s that include tests need to be located. Such pull requests can be located by querying for matching keywords (like ‘test’, ‘unit-test’ etc.) in the columns ‘title’ and/or ‘body’ in the pull_request table, or for similar keywords in the ‘message’ column of the pr_commits table (with possibility of many commits per single PR). The total records of testing PR’s can be compared to the total number of PRs to obtain metrics on frequency (test to code churn ratio). Individual metrics for individual repos or individual AI Agent ‘users’ may also be calculated for further analysis (by isolating on user ID and/or repo ID). If time permits, then we may go further to compare record counts for PR’s that match specific test types by their keywords in the commit or PR descriptions, or if follow up tests were requested from an agent’s PR (ex. examining pr_reviews and comments with a ‘CHANGES_REQUESTED’ state and a body that requests tests). Pie charts and graphs will be constructed in python to help visualize and compare these various metrics and form our conclusions.
